#!/usr/bin/env python3
"""
YouTubeå…¨å‹•ç”»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«: èƒ¡ç”°æ˜Œå½¦ / Windows, Azure, M365, ç”ŸæˆAI (UCn_7IV61pGOfoiC5Lc8nHUw)
å‡ºåŠ›: ../data/videos.json, ../data/playlists.json

æ—¢å­˜ã® obsidian/scripts/youtube_scraper.py ã®OAuthèªè¨¼ã‚’å†åˆ©ç”¨ã™ã‚‹ã€‚
"""

import json
import os
import re
import sys
from datetime import datetime, timezone
from pathlib import Path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build

CHANNEL_ID = "UCn_7IV61pGOfoiC5Lc8nHUw"
OBSIDIAN_SCRIPTS = Path("/home/ebi/obsidian/scripts")
OUTPUT_DIR = Path(__file__).resolve().parent.parent / "data"
OBSIDIAN_VIDEOS_DIR = Path("/home/ebi/obsidian/05_Resources/YouTubeå‹•ç”»")

OAUTH_SCOPES = [
    "https://www.googleapis.com/auth/youtube.readonly",
    "https://www.googleapis.com/auth/youtube.force-ssl",
]


def get_youtube_service():
    """OAuthèªè¨¼ã§YouTubeã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—ï¼ˆæ—¢å­˜ãƒˆãƒ¼ã‚¯ãƒ³å†åˆ©ç”¨ï¼‰"""
    token_file = OBSIDIAN_SCRIPTS / "token.json"
    credentials = Credentials.from_authorized_user_file(str(token_file), OAUTH_SCOPES)

    if credentials.refresh_token:
        credentials.refresh(Request())
        with open(token_file, "w") as f:
            f.write(credentials.to_json())

    return build("youtube", "v3", credentials=credentials)


def get_api_key_service():
    """APIã‚­ãƒ¼èªè¨¼ã®ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆå†ç”Ÿæ•°ç­‰ã®çµ±è¨ˆå–å¾—ç”¨ï¼‰"""
    # .envã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
    env_file = OBSIDIAN_SCRIPTS / ".env"
    api_key = None
    for line in env_file.read_text().splitlines():
        if line.startswith("YOUTUBE_API_KEY="):
            api_key = line.split("=", 1)[1].strip().strip('"').strip("'")
            break
    if not api_key:
        raise ValueError("YOUTUBE_API_KEY not found in .env")
    return build("youtube", "v3", developerKey=api_key)


def parse_duration(iso_duration: str) -> int:
    """ISO 8601 duration (PT1H2M3S) ã‚’ç§’æ•°ã«å¤‰æ›"""
    if not iso_duration:
        return 0
    match = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", iso_duration)
    if not match:
        return 0
    h, m, s = (int(g) if g else 0 for g in match.groups())
    return h * 3600 + m * 60 + s


def format_duration(seconds: int) -> str:
    """ç§’æ•°ã‚’ HH:MM:SS or MM:SS ã«å¤‰æ›"""
    h, remainder = divmod(seconds, 3600)
    m, s = divmod(remainder, 60)
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"


def fetch_all_video_ids(yt_oauth) -> list[dict]:
    """OAuthèªè¨¼ã§å…¨å‹•ç”»ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆçµŒç”±ï¼‰"""
    # ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆIDå–å¾—
    ch_resp = yt_oauth.channels().list(
        part="contentDetails,snippet", id=CHANNEL_ID
    ).execute()

    if not ch_resp.get("items"):
        raise ValueError(f"Channel not found: {CHANNEL_ID}")

    channel = ch_resp["items"][0]
    uploads_id = channel["contentDetails"]["relatedPlaylists"]["uploads"]
    print(f"ğŸ“º ãƒãƒ£ãƒ³ãƒãƒ«: {channel['snippet']['title']}")
    print(f"ğŸ“‹ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰PL: {uploads_id}")

    videos = []
    next_token = None
    while True:
        resp = yt_oauth.playlistItems().list(
            part="snippet,contentDetails",
            playlistId=uploads_id,
            maxResults=50,
            pageToken=next_token,
        ).execute()

        for item in resp.get("items", []):
            vid = item["contentDetails"].get("videoId")
            if not vid:
                continue
            snip = item["snippet"]
            # ä»–ãƒãƒ£ãƒ³ãƒãƒ«ã®å‹•ç”»ã‚’ã‚¹ã‚­ãƒƒãƒ—
            owner_id = snip.get("videoOwnerChannelId", "")
            if owner_id and owner_id != CHANNEL_ID:
                continue

            thumbnails = snip.get("thumbnails", {})
            thumb = (
                thumbnails.get("high", {}).get("url")
                or thumbnails.get("medium", {}).get("url")
                or thumbnails.get("default", {}).get("url", "")
            )

            videos.append({
                "videoId": vid,
                "title": snip.get("title", ""),
                "description": snip.get("description", ""),
                "publishedAt": snip.get("publishedAt", ""),
                "thumbnailUrl": thumb,
            })

        next_token = resp.get("nextPageToken")
        if not next_token:
            break
        print(f"  å–å¾—ä¸­... {len(videos)}æœ¬")

    print(f"âœ… å…¨{len(videos)}æœ¬ã®å‹•ç”»IDã‚’å–å¾—")
    return videos


def enrich_with_statistics(yt_api, videos: list[dict]) -> list[dict]:
    """APIã‚­ãƒ¼èªè¨¼ã§å†ç”Ÿæ•°ãƒ»ã„ã„ã­æ•°ãƒ»å†ç”Ÿæ™‚é–“ç­‰ã‚’è¿½åŠ å–å¾—ï¼ˆ50ä»¶ãšã¤ãƒãƒƒãƒï¼‰"""
    print("ğŸ“Š çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ä¸­...")
    for i in range(0, len(videos), 50):
        batch = videos[i : i + 50]
        ids = ",".join(v["videoId"] for v in batch)
        resp = yt_api.videos().list(
            part="statistics,contentDetails,status", id=ids
        ).execute()

        stats_map = {}
        for item in resp.get("items", []):
            stats_map[item["id"]] = item

        for v in batch:
            detail = stats_map.get(v["videoId"])
            if not detail:
                v["viewCount"] = 0
                v["likeCount"] = 0
                v["durationSeconds"] = 0
                v["duration"] = "0:00"
                v["privacyStatus"] = "unknown"
                v["tags"] = []
                continue

            stats = detail.get("statistics", {})
            content = detail.get("contentDetails", {})
            status = detail.get("status", {})

            dur_sec = parse_duration(content.get("duration", ""))
            v["viewCount"] = int(stats.get("viewCount", 0))
            v["likeCount"] = int(stats.get("likeCount", 0))
            v["durationSeconds"] = dur_sec
            v["duration"] = format_duration(dur_sec)
            v["privacyStatus"] = status.get("privacyStatus", "unknown")
            v["tags"] = detail.get("snippet", {}).get("tags", [])

        print(f"  {min(i + 50, len(videos))}/{len(videos)}")

    return videos


def fetch_all_playlists(yt_api) -> list[dict]:
    """å…¨ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ + æ‰€å±å‹•ç”»IDã‚’å–å¾—"""
    print("\nğŸ“‹ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—ä¸­...")
    playlists = []
    next_token = None

    while True:
        resp = yt_api.playlists().list(
            part="snippet,contentDetails",
            channelId=CHANNEL_ID,
            maxResults=50,
            pageToken=next_token,
        ).execute()

        for item in resp.get("items", []):
            playlists.append({
                "playlistId": item["id"],
                "title": item["snippet"]["title"],
                "description": item["snippet"].get("description", ""),
                "itemCount": item["contentDetails"]["itemCount"],
                "publishedAt": item["snippet"]["publishedAt"],
                "videoIds": [],
            })

        next_token = resp.get("nextPageToken")
        if not next_token:
            break

    print(f"âœ… {len(playlists)}å€‹ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’å–å¾—")

    # å„ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆå†…ã®å‹•ç”»IDã‚’å–å¾—
    for pl in playlists:
        if pl["itemCount"] == 0:
            continue
        video_ids = []
        next_token = None
        while True:
            resp = yt_api.playlistItems().list(
                part="contentDetails",
                playlistId=pl["playlistId"],
                maxResults=50,
                pageToken=next_token,
            ).execute()
            for item in resp.get("items", []):
                vid = item["contentDetails"].get("videoId")
                if vid:
                    video_ids.append(vid)
            next_token = resp.get("nextPageToken")
            if not next_token:
                break
        pl["videoIds"] = video_ids
        print(f"  [{len(video_ids)}æœ¬] {pl['title']}")

    return playlists


def link_obsidian_notes(videos: list[dict]) -> list[dict]:
    """Obsidianã®æ—¢å­˜ãƒãƒ¼ãƒˆã‹ã‚‰videoIdã‚’æŠ½å‡ºã—ã¦æ–‡å­—èµ·ã“ã—ã‚’ç´ä»˜ã‘"""
    print("\nğŸ“ Obsidianãƒãƒ¼ãƒˆã‚’ç´ä»˜ã‘ä¸­...")

    if not OBSIDIAN_VIDEOS_DIR.exists():
        print("  âš ï¸ Obsidian YouTubeå‹•ç”»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return videos

    # ãƒãƒ¼ãƒˆã‹ã‚‰videoIdâ†’æ–‡å­—èµ·ã“ã—ã®ãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
    transcript_map = {}
    for note_path in OBSIDIAN_VIDEOS_DIR.glob("*.md"):
        content = note_path.read_text(encoding="utf-8", errors="replace")
        # URLã‹ã‚‰videoIdã‚’æŠ½å‡º
        match = re.search(r"youtube\.com/watch\?v=([a-zA-Z0-9_-]+)", content)
        if not match:
            continue
        vid = match.group(1)

        # æ–‡å­—èµ·ã“ã—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        transcript_match = re.search(
            r"## æ–‡å­—èµ·ã“ã—.*?\n(.*)",
            content,
            re.DOTALL,
        )
        if transcript_match:
            transcript = transcript_match.group(1).strip()
            # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ï¼ˆæ¦‚è¦ç”Ÿæˆç”¨ãªã®ã§å…ˆé ­ã§ååˆ†ï¼‰
            if len(transcript) > 3000:
                transcript = transcript[:3000] + "..."
            transcript_map[vid] = transcript

    # å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã«ç´ä»˜ã‘
    linked = 0
    for v in videos:
        transcript = transcript_map.get(v["videoId"])
        if transcript:
            v["transcript"] = transcript
            linked += 1
        else:
            v["transcript"] = ""

    print(f"âœ… {linked}/{len(videos)}æœ¬ã®å‹•ç”»ã«æ–‡å­—èµ·ã“ã—ã‚’ç´ä»˜ã‘")
    return videos


def build_reverse_playlist_map(playlists: list[dict]) -> dict:
    """å‹•ç”»ID â†’ æ‰€å±ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆåãƒªã‚¹ãƒˆã®é€†å¼•ããƒãƒƒãƒ—"""
    reverse = {}
    for pl in playlists:
        for vid in pl["videoIds"]:
            if vid not in reverse:
                reverse[vid] = []
            reverse[vid].append(pl["title"])
    return reverse


def main():
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    print("ğŸš€ å…¨å‹•ç”»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹\n")

    # 1. OAuthèªè¨¼ã§å…¨å‹•ç”»ID+åŸºæœ¬æƒ…å ±å–å¾—
    yt_oauth = get_youtube_service()
    videos = fetch_all_video_ids(yt_oauth)

    # 2. APIã‚­ãƒ¼èªè¨¼ã§çµ±è¨ˆæƒ…å ±è¿½åŠ 
    yt_api = get_api_key_service()
    videos = enrich_with_statistics(yt_api, videos)

    # 3. ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆå–å¾—
    playlists = fetch_all_playlists(yt_api)

    # 4. Obsidianãƒãƒ¼ãƒˆç´ä»˜ã‘
    videos = link_obsidian_notes(videos)

    # 5. ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆé€†å¼•ãã‚’å‹•ç”»ã«è¿½åŠ 
    reverse_map = build_reverse_playlist_map(playlists)
    for v in videos:
        v["playlists"] = reverse_map.get(v["videoId"], [])

    # 6. å…¬é–‹æ—¥ã§æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
    videos.sort(key=lambda v: v.get("publishedAt", ""), reverse=True)

    # 7. ä¿å­˜
    videos_path = OUTPUT_DIR / "videos.json"
    with open(videos_path, "w", encoding="utf-8") as f:
        json.dump(videos, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ {videos_path} ã«{len(videos)}æœ¬ã®å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")

    playlists_path = OUTPUT_DIR / "playlists.json"
    with open(playlists_path, "w", encoding="utf-8") as f:
        json.dump(playlists, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ {playlists_path} ã«{len(playlists)}å€‹ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")

    # ã‚µãƒãƒªãƒ¼
    total_views = sum(v.get("viewCount", 0) for v in videos)
    total_duration_h = sum(v.get("durationSeconds", 0) for v in videos) / 3600
    with_transcript = sum(1 for v in videos if v.get("transcript"))
    public = sum(1 for v in videos if v.get("privacyStatus") == "public")
    unlisted = sum(1 for v in videos if v.get("privacyStatus") == "unlisted")
    private = sum(1 for v in videos if v.get("privacyStatus") == "private")

    print(f"\nğŸ“Š ã‚µãƒãƒªãƒ¼:")
    print(f"  å‹•ç”»æ•°: {len(videos)}æœ¬ï¼ˆå…¬é–‹{public} / é™å®š{unlisted} / éå…¬é–‹{private}ï¼‰")
    print(f"  ç·å†ç”Ÿæ•°: {total_views:,}")
    print(f"  ç·å†ç”Ÿæ™‚é–“: {total_duration_h:.1f}æ™‚é–“")
    print(f"  æ–‡å­—èµ·ã“ã—ä»˜ã: {with_transcript}æœ¬")
    print(f"  ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ: {len(playlists)}å€‹")


if __name__ == "__main__":
    main()
